{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "177050af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | Num workers: 8 | Pin memory: True\n",
      "üì• Loading datasets...\n",
      "Loaded train: (179951, 26) | test: (179951, 26)\n",
      "üîß Applying basic time feature extraction...\n",
      "‚úÖ Preprocessing complete. Train shape: (179951, 30) | Test shape: (179951, 30)\n",
      "Train stealth count: 1343 | Test stealth count: 1343\n",
      "üöÄ Running Isolation Forest anomaly detection...\n",
      "‚ö†Ô∏è Found non-numeric columns: ['privilege_level'] ‚Äî encoding now...\n",
      "‚úÖ All columns converted to numeric.\n",
      "‚úÖ Anomaly detection complete.\n",
      "Train graph nodes: 314, Test graph nodes: 314\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.data import Data as GeoData\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Try Captum for IG; fallback to grad*input\n",
    "try:\n",
    "    from captum.attr import IntegratedGradients\n",
    "    HAS_CAPTUM = True\n",
    "except Exception:\n",
    "    HAS_CAPTUM = False\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "TRAIN_DATA_PATH = \"slm_train_dataset.csv\"\n",
    "TEST_DATA_PATH  = \"slm_test_dataset.csv\"\n",
    "BATCH_SIZE = 64\n",
    "THRESHOLD = 0.05   # 30% of sequences must be predicted malicious\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "SEQ_LENGTH = 20  # Increased from 10\n",
    "ANOMALY_WEIGHT_ALPHA = 1.5  # Reduced from 2\n",
    "GNN_NODE_FEAT_DIM = 32\n",
    "GNN_OUT_DIM = 64\n",
    "LSTM_HIDDEN = 64\n",
    "ATTN_DIM = 64\n",
    "PIN_MEMORY = True\n",
    "SEQUENCE_LABELING_STRATEGY = \"threshold\"   # \"last\", \"threshold\", or \"weighted\"\n",
    "NUM_WORKERS = min(8, max(1, (os.cpu_count() or 4) // 2))\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "PRINT_PROGRESS = True\n",
    "SAVE_MODEL_PATH = \"slm_enhanced_model.pth\"\n",
    "\n",
    "print(f\"Device: {DEVICE} | Num workers: {NUM_WORKERS} | Pin memory: {PIN_MEMORY}\")\n",
    "\n",
    "# -------------------------\n",
    "# UTILS: reproducibility\n",
    "# -------------------------\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    # higher precision for matmul on modern NVIDIA\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# -------------------------\n",
    "# 1) LOAD DUAL DATASETS\n",
    "# -------------------------\n",
    "if not os.path.exists(TRAIN_DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Train dataset not found at: {TRAIN_DATA_PATH}\")\n",
    "if not os.path.exists(TEST_DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Test dataset not found at: {TEST_DATA_PATH}\")\n",
    "\n",
    "print(\"üì• Loading datasets...\")\n",
    "df_train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "df_test  = pd.read_csv(TEST_DATA_PATH)\n",
    "print(f\"Loaded train: {df_train.shape} | test: {df_test.shape}\")\n",
    "\n",
    "# -------------------------\n",
    "# 2) PREPROCESS (fit on train, apply to test)\n",
    "# -------------------------\n",
    "def basic_time_features(df):\n",
    "    df = df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    df['hour'] = df['timestamp'].dt.hour.fillna(0).astype(int)\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek.fillna(0).astype(int)\n",
    "    df['day'] = df['timestamp'].dt.day.fillna(0).astype(int)\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5,6]).astype(int)\n",
    "    df['is_night'] = ((df['hour'] < 6) | (df['hour'] > 22)).astype(int)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "print(\"üîß Applying basic time feature extraction...\")\n",
    "df_train = basic_time_features(df_train)\n",
    "df_test  = basic_time_features(df_test)\n",
    "\n",
    "# categorical columns (update to match your CSV)\n",
    "cat_cols = [\n",
    "    'host_id', 'host_hostname', 'host_id_unique', 'user_id',\n",
    "    'src_ip', 'dst_ip', 'dst_host', 'event_type', 'process_name',\n",
    "    'parent_process', 'command_line_args', 'auth_result', 'auth_type',\n",
    "    'logon_type', 'protocol', 'zone_src', 'zone_dst', 'attack_stage', 'session_id'\n",
    "]\n",
    "\n",
    "# Fit LabelEncoders on training data and apply to test\n",
    "encoders = {}\n",
    "for col in cat_cols:\n",
    "    if col in df_train.columns:\n",
    "        le = LabelEncoder()\n",
    "        # fit on combined train values to be robust (train primary, but include unseen in test)\n",
    "        le.fit(df_train[col].astype(str).tolist() + df_test.get(col, pd.Series(dtype=str)).astype(str).tolist())\n",
    "        df_train[col] = le.transform(df_train[col].astype(str))\n",
    "        if col in df_test.columns:\n",
    "            # transform test; unseen values are handled because we fit on combined above\n",
    "            df_test[col] = le.transform(df_test[col].astype(str))\n",
    "        encoders[col] = le\n",
    "\n",
    "# Ensure label column exists\n",
    "if 'is_stealth_lateral_movement' not in df_train.columns or 'is_stealth_lateral_movement' not in df_test.columns:\n",
    "    raise ValueError(\"Both datasets must contain 'is_stealth_lateral_movement' column\")\n",
    "\n",
    "# Split X/y\n",
    "y_train = df_train['is_stealth_lateral_movement'].astype(int)\n",
    "X_train = df_train.drop(columns=['is_stealth_lateral_movement'])\n",
    "y_test = df_test['is_stealth_lateral_movement'].astype(int)\n",
    "X_test = df_test.drop(columns=['is_stealth_lateral_movement'])\n",
    "\n",
    "# Numeric scaling: fit StandardScaler on train and apply to test\n",
    "num_cols = ['port','bytes_in','bytes_out','hour','day_of_week','day']\n",
    "scaler = StandardScaler()\n",
    "for col in num_cols:\n",
    "    if col in X_train.columns:\n",
    "        # Fit only on train\n",
    "        X_train[col] = scaler.fit_transform(X_train[[col]])\n",
    "        if col in X_test.columns:\n",
    "            X_test[col] = scaler.transform(X_test[[col]])\n",
    "\n",
    "print(f\"‚úÖ Preprocessing complete. Train shape: {X_train.shape} | Test shape: {X_test.shape}\")\n",
    "print(f\"Train stealth count: {int(y_train.sum())} | Test stealth count: {int(y_test.sum())}\")\n",
    "\n",
    "# ==========================================================\n",
    "# 3) SAFE ANOMALY DETECTION (auto-encode leftover strings)\n",
    "# ==========================================================\n",
    "print('üöÄ Running Isolation Forest anomaly detection...')\n",
    "\n",
    "X_train_clean = X_train.drop(columns=['timestamp'], errors='ignore').copy()\n",
    "X_test_clean  = X_test.drop(columns=['timestamp'], errors='ignore').copy()\n",
    "\n",
    "# Detect any non-numeric columns and encode them (safe fallback)\n",
    "non_numeric_cols = X_train_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "if non_numeric_cols:\n",
    "    print(f\"‚ö†Ô∏è Found non-numeric columns: {non_numeric_cols} ‚Äî encoding now...\")\n",
    "    for col in non_numeric_cols:\n",
    "        le = LabelEncoder()\n",
    "        combined = list(X_train_clean[col].astype(str)) + list(X_test_clean.get(col, pd.Series(dtype=str)).astype(str))\n",
    "        le.fit(combined)\n",
    "        X_train_clean[col] = le.transform(X_train_clean[col].astype(str))\n",
    "        if col in X_test_clean.columns:\n",
    "            X_test_clean[col] = le.transform(X_test_clean[col].astype(str))\n",
    "    print(\"‚úÖ All columns converted to numeric.\")\n",
    "\n",
    "# Verify numeric-only data\n",
    "assert all(pd.api.types.is_numeric_dtype(X_train_clean[col]) for col in X_train_clean.columns), \\\n",
    "    \"Non-numeric data found even after encoding!\"\n",
    "\n",
    "# Run Isolation Forest on train-clean only\n",
    "iso = IsolationForest(n_estimators=200, contamination=0.03, random_state=42, n_jobs=-1)\n",
    "iso.fit(X_train_clean)\n",
    "X_train['anomaly_score'] = -iso.decision_function(X_train_clean)\n",
    "X_test['anomaly_score'] = -iso.decision_function(X_test_clean)\n",
    "print(\"‚úÖ Anomaly detection complete.\")\n",
    "\n",
    "# ==========================================================\n",
    "# 4) GRAPH PREPARATION\n",
    "# ==========================================================\n",
    "def build_graph(X_part):\n",
    "    edge_index = []\n",
    "    # ensure host_id and user_id exist\n",
    "    if 'host_id' not in X_part.columns or 'user_id' not in X_part.columns:\n",
    "        # fallback: create minimal graph with one node\n",
    "        node_features = torch.randn(1, GNN_NODE_FEAT_DIM, dtype=torch.float32)\n",
    "        edge_index = torch.tensor([[0],[0]], dtype=torch.long)\n",
    "        g = GeoData(x=node_features, edge_index=edge_index)\n",
    "        g.host_map = {}\n",
    "        g.user_map = {}\n",
    "        g.hosts = []\n",
    "        g.users = []\n",
    "        return g\n",
    "\n",
    "    hosts = np.unique(X_part['host_id'].astype(int))\n",
    "    users = np.unique(X_part['user_id'].astype(int))\n",
    "    host_map = {int(h): i for i, h in enumerate(hosts)}\n",
    "    user_map = {int(u): len(hosts) + i for i, u in enumerate(users)}\n",
    "\n",
    "    for _, row in X_part.iterrows():\n",
    "        try:\n",
    "            h_val = int(row['host_id'])\n",
    "            u_val = int(row['user_id'])\n",
    "        except Exception:\n",
    "            continue\n",
    "        if h_val not in host_map or u_val not in user_map:\n",
    "            continue\n",
    "        h = host_map[h_val]\n",
    "        u = user_map[u_val]\n",
    "        edge_index.append([u, h]); edge_index.append([h, u])\n",
    "\n",
    "    if len(edge_index) == 0:\n",
    "        edge_index = [[0,0]]\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    n_nodes = len(hosts) + len(users)\n",
    "    node_features = torch.randn(n_nodes, GNN_NODE_FEAT_DIM, dtype=torch.float32)\n",
    "    g = GeoData(x=node_features, edge_index=edge_index)\n",
    "    g.host_map = host_map; g.user_map = user_map; g.hosts = hosts; g.users = users\n",
    "    return g\n",
    "\n",
    "train_graph = build_graph(X_train)\n",
    "test_graph = build_graph(X_test)\n",
    "print(f\"Train graph nodes: {train_graph.x.shape[0]}, Test graph nodes: {test_graph.x.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb1ed6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating sequences...\n",
      "‚úÖ Sequences built: 178051 | Features: 30 | Seq len: 20\n",
      "‚úÖ Sequences built: 178051 | Features: 30 | Seq len: 20\n",
      "Train seqs: torch.Size([178051, 20, 30]), Test seqs: torch.Size([178051, 20, 30])\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 5) SAFE SEQUENCE CREATION (robust host-level builder)\n",
    "# ==========================================================\n",
    "def create_sequences(X_data, y_data, seq_length=SEQ_LENGTH, graph=None):\n",
    "    \"\"\"Build overlapping sequences per host with anomaly mean tracking.\"\"\"\n",
    "    if 'host_id' not in X_data.columns:\n",
    "        raise KeyError(\"Column 'host_id' not found in X_data ‚Äî check preprocessing.\")\n",
    "\n",
    "    X_data_clean = X_data.drop(columns=['timestamp'], errors='ignore').copy()\n",
    "    # Ensure numeric conversion for safety\n",
    "    for col in X_data_clean.columns:\n",
    "        if X_data_clean[col].dtype == 'object':\n",
    "            X_data_clean[col] = pd.factorize(X_data_clean[col])[0]\n",
    "\n",
    "    if 'anomaly_score' not in X_data_clean.columns:\n",
    "        X_data_clean['anomaly_score'] = 0.0\n",
    "\n",
    "    X_data_clean = X_data_clean.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    sequences, labels, host_node_idxs, seq_anom_means = [], [], [], []\n",
    "    host_map = getattr(graph, 'host_map', None) if graph is not None else {}\n",
    "\n",
    "    unique_hosts = X_data_clean['host_id'].unique()\n",
    "    if len(unique_hosts) == 0:\n",
    "        raise ValueError(\"No hosts found in data (check host_id column).\")\n",
    "\n",
    "    for host_id in unique_hosts:\n",
    "        host_mask = X_data_clean['host_id'] == host_id\n",
    "        host_df = X_data_clean.loc[host_mask]\n",
    "        if host_df.empty:\n",
    "            continue\n",
    "\n",
    "        host_arr = host_df.values\n",
    "        host_labels = y_data.loc[host_mask].values if isinstance(y_data, pd.Series) else y_data[host_mask].values\n",
    "        host_anom = host_df['anomaly_score'].values  # ‚úÖ fixed line\n",
    "\n",
    "        if np.isnan(host_anom).any():\n",
    "            host_anom = np.nan_to_num(host_anom, nan=0.0)\n",
    "\n",
    "        # fallback for small hosts\n",
    "        if len(host_arr) < seq_length:\n",
    "            for i in range(len(host_arr)):\n",
    "                sequences.append(host_arr[i:i+1])\n",
    "                labels.append(int(host_labels[i]))\n",
    "                idx = host_map.get(int(host_id), 0)\n",
    "                host_node_idxs.append(idx)\n",
    "                seq_anom_means.append(float(np.mean(host_anom[max(i, 0):i+1])))\n",
    "            continue\n",
    "\n",
    "        # sliding window\n",
    "        # sliding window\n",
    "        for i in range(len(host_arr) - seq_length + 1):\n",
    "            seq = host_arr[i:i+seq_length]\n",
    "            sequences.append(seq)\n",
    "\n",
    "            window_labels = host_labels[i:i+seq_length]\n",
    "            window_anoms  = host_anom[i:i+seq_length]\n",
    "\n",
    "            # ---- CHOOSE LABELING METHOD ----\n",
    "            if SEQUENCE_LABELING_STRATEGY == \"last\":\n",
    "                # Option A\n",
    "                label = int(window_labels[-1])\n",
    "\n",
    "            elif SEQUENCE_LABELING_STRATEGY == \"threshold\":\n",
    "                # Option B\n",
    "                label = 1 if np.sum(window_labels) >= 2 else 0\n",
    "\n",
    "            elif SEQUENCE_LABELING_STRATEGY == \"weighted\":\n",
    "                # Option C\n",
    "                mal_score = np.sum(window_labels * (1 + window_anoms))\n",
    "                label = 1 if mal_score > 1.5 else 0\n",
    "\n",
    "            else:\n",
    "                # default fallback to old method (not recommended)\n",
    "                label = int(np.max(window_labels))\n",
    "\n",
    "            labels.append(label)\n",
    "\n",
    "            # other fields remain same\n",
    "            idx = host_map.get(int(host_id), 0)\n",
    "            host_node_idxs.append(idx)\n",
    "            seq_anom_means.append(float(np.mean(window_anoms)))\n",
    "\n",
    "\n",
    "    if len(sequences) == 0:\n",
    "        raise ValueError(\"No sequences created ‚Äî check data content and host_id mapping.\")\n",
    "\n",
    "    # Pad sequences for equal length (should already be seq_length)\n",
    "    max_len = max(len(s) for s in sequences)\n",
    "    feat_dim = sequences[0].shape[1]\n",
    "    padded = []\n",
    "    for s in sequences:\n",
    "        if len(s) < max_len:\n",
    "            pad = np.zeros((max_len - len(s), feat_dim))\n",
    "            s = np.vstack([s, pad])\n",
    "        padded.append(s)\n",
    "\n",
    "    sequences = np.array(padded, dtype=np.float32)\n",
    "    labels = np.array(labels, dtype=np.int64)\n",
    "    host_node_idxs = np.array(host_node_idxs, dtype=np.int64)\n",
    "    seq_anom_means = np.array(seq_anom_means, dtype=np.float32)\n",
    "\n",
    "    print(f\"‚úÖ Sequences built: {len(sequences)} | Features: {feat_dim} | Seq len: {max_len}\")\n",
    "    return (\n",
    "        torch.tensor(sequences, dtype=torch.float32),\n",
    "        torch.tensor(labels, dtype=torch.long),\n",
    "        torch.tensor(host_node_idxs, dtype=torch.long),\n",
    "        torch.tensor(seq_anom_means, dtype=torch.float32),\n",
    "    )\n",
    "\n",
    "print(\"üîÑ Creating sequences...\")\n",
    "X_train_seq, y_train_seq, train_host_node_idxs, train_seq_anom_mean = create_sequences(\n",
    "    X_train, y_train, seq_length=SEQ_LENGTH, graph=train_graph\n",
    ")\n",
    "X_test_seq, y_test_seq, test_host_node_idxs, test_seq_anom_mean = create_sequences(\n",
    "    X_test, y_test, seq_length=SEQ_LENGTH, graph=test_graph\n",
    ")\n",
    "\n",
    "print(f\"Train seqs: {X_train_seq.shape}, Test seqs: {X_test_seq.shape}\")\n",
    "\n",
    "train_dataset = TensorDataset(X_train_seq, y_train_seq, train_host_node_idxs, train_seq_anom_mean)\n",
    "test_dataset  = TensorDataset(X_test_seq,  y_test_seq,  test_host_node_idxs,  test_seq_anom_mean)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    drop_last=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b72bf2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 6) Model components\n",
    "# -------------------------\n",
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(self, in_feats=GNN_NODE_FEAT_DIM, hidden=64, out_feats=GNN_OUT_DIM):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_feats, hidden)\n",
    "        self.conv2 = GCNConv(hidden, out_feats)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = torch.relu(self.conv2(x, edge_index))\n",
    "        return x\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, attn_dim=ATTN_DIM):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(nn.Linear(hidden_dim, attn_dim), nn.Tanh(), nn.Linear(attn_dim, 1))\n",
    "    def forward(self, lstm_out):\n",
    "        weights = self.proj(lstm_out)  # (b, seq, 1)\n",
    "        weights = torch.softmax(weights, dim=1)\n",
    "        weighted = (lstm_out * weights).sum(dim=1)\n",
    "        return weighted, weights.squeeze(-1)\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes=2, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=bidirectional)\n",
    "        factor = 2 if bidirectional else 1\n",
    "        self.attn = SelfAttention(hidden_dim * factor, attn_dim=ATTN_DIM)\n",
    "        self.fc = nn.Linear(hidden_dim * factor, num_classes)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        weighted, attn_weights = self.attn(out)\n",
    "        logits = self.fc(weighted)\n",
    "        return logits, attn_weights, out\n",
    "\n",
    "class StealthLateralModel(nn.Module):\n",
    "    def __init__(self, input_dim, gnn_out_dim=GNN_OUT_DIM, lstm_hidden=LSTM_HIDDEN):\n",
    "        super().__init__()\n",
    "        self.gnn = GNNEncoder(in_feats=GNN_NODE_FEAT_DIM, out_feats=gnn_out_dim)\n",
    "        self.lstm = BiLSTMModel(input_dim + gnn_out_dim, hidden_dim=lstm_hidden, num_classes=2)\n",
    "    def forward(self, graph_data, seq_batch, host_node_idxs, seq_anom_means):\n",
    "        # compute gnn embeddings\n",
    "        gnn_out = self.gnn(graph_data.x, graph_data.edge_index)  # (n_nodes, gnn_out_dim)\n",
    "        # gather host node embeddings\n",
    "        node_embeds = gnn_out[host_node_idxs]  # (batch, gnn_out_dim)\n",
    "        batch_size, seq_len, _ = seq_batch.shape\n",
    "        node_embeds_expanded = node_embeds.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        # normalize and weight anomaly\n",
    "        if seq_anom_means.numel() > 1:\n",
    "            norm = (seq_anom_means - seq_anom_means.min()) / (seq_anom_means.max() - seq_anom_means.min() + 1e-6)\n",
    "        else:\n",
    "            norm = seq_anom_means * 0.0\n",
    "        weights = (1.0 + ANOMALY_WEIGHT_ALPHA * norm).unsqueeze(-1).unsqueeze(-1)\n",
    "        node_embeds_expanded = node_embeds_expanded * weights.to(node_embeds_expanded.device)\n",
    "        seq_with_context = torch.cat([seq_batch, node_embeds_expanded.to(seq_batch.dtype)], dim=-1)\n",
    "        logits, attn_weights, lstm_outs = self.lstm(seq_with_context)\n",
    "        return logits, attn_weights, lstm_outs, gnn_out\n",
    "\n",
    "# instantiate model\n",
    "input_dim = X_train_seq.shape[-1]\n",
    "model = StealthLateralModel(input_dim=input_dim, gnn_out_dim=GNN_OUT_DIM, lstm_hidden=LSTM_HIDDEN).to(DEVICE)\n",
    "print(\"Model created.\")\n",
    "\n",
    "# class weights for imbalance (on sequence-level)\n",
    "counts = np.bincount(y_train_seq.numpy())\n",
    "if len(counts) == 1:\n",
    "    class_weights = torch.tensor([1.0,1.0], dtype=torch.float32).to(DEVICE)\n",
    "else:\n",
    "    pos_weight = (counts.sum() / (2.0 * counts[1])) if counts[1] > 0 else 1.0\n",
    "    class_weights = torch.tensor([1.0, pos_weight], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# multi-GPU guard (your 4050 is single GPU; this is safe)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    print(\"DataParallel mode enabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca4acf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training...\n",
      "Epoch 1/10 | Step 50/2782 | Loss 0.7630\n",
      "Epoch 1/10 | Step 100/2782 | Loss 0.6286\n",
      "Epoch 1/10 | Step 150/2782 | Loss 0.7190\n",
      "Epoch 1/10 | Step 200/2782 | Loss 0.8499\n",
      "Epoch 1/10 | Step 250/2782 | Loss 0.3430\n",
      "Epoch 1/10 | Step 300/2782 | Loss 0.4632\n",
      "Epoch 1/10 | Step 350/2782 | Loss 0.5977\n",
      "Epoch 1/10 | Step 400/2782 | Loss 0.5705\n",
      "Epoch 1/10 | Step 450/2782 | Loss 0.4409\n",
      "Epoch 1/10 | Step 500/2782 | Loss 0.5561\n",
      "Epoch 1/10 | Step 550/2782 | Loss 0.4494\n",
      "Epoch 1/10 | Step 600/2782 | Loss 0.6630\n",
      "Epoch 1/10 | Step 650/2782 | Loss 0.6918\n",
      "Epoch 1/10 | Step 700/2782 | Loss 0.4796\n",
      "Epoch 1/10 | Step 750/2782 | Loss 0.4891\n",
      "Epoch 1/10 | Step 800/2782 | Loss 0.3747\n",
      "Epoch 1/10 | Step 850/2782 | Loss 0.5481\n",
      "Epoch 1/10 | Step 900/2782 | Loss 0.4507\n",
      "Epoch 1/10 | Step 950/2782 | Loss 0.5592\n",
      "Epoch 1/10 | Step 1000/2782 | Loss 0.3889\n",
      "Epoch 1/10 | Step 1050/2782 | Loss 0.4160\n",
      "Epoch 1/10 | Step 1100/2782 | Loss 0.6131\n",
      "Epoch 1/10 | Step 1150/2782 | Loss 0.5646\n",
      "Epoch 1/10 | Step 1200/2782 | Loss 0.7948\n",
      "Epoch 1/10 | Step 1250/2782 | Loss 0.5317\n",
      "Epoch 1/10 | Step 1300/2782 | Loss 0.5112\n",
      "Epoch 1/10 | Step 1350/2782 | Loss 0.3195\n",
      "Epoch 1/10 | Step 1400/2782 | Loss 0.2100\n",
      "Epoch 1/10 | Step 1450/2782 | Loss 0.5795\n",
      "Epoch 1/10 | Step 1500/2782 | Loss 0.6707\n",
      "Epoch 1/10 | Step 1550/2782 | Loss 0.4854\n",
      "Epoch 1/10 | Step 1600/2782 | Loss 0.7369\n",
      "Epoch 1/10 | Step 1650/2782 | Loss 0.6069\n",
      "Epoch 1/10 | Step 1700/2782 | Loss 0.5817\n",
      "Epoch 1/10 | Step 1750/2782 | Loss 0.1856\n",
      "Epoch 1/10 | Step 1800/2782 | Loss 0.4065\n",
      "Epoch 1/10 | Step 1850/2782 | Loss 0.5169\n",
      "Epoch 1/10 | Step 1900/2782 | Loss 0.2567\n",
      "Epoch 1/10 | Step 1950/2782 | Loss 0.3765\n",
      "Epoch 1/10 | Step 2000/2782 | Loss 0.3706\n",
      "Epoch 1/10 | Step 2050/2782 | Loss 0.3294\n",
      "Epoch 1/10 | Step 2100/2782 | Loss 0.2315\n",
      "Epoch 1/10 | Step 2150/2782 | Loss 0.1726\n",
      "Epoch 1/10 | Step 2200/2782 | Loss 0.4726\n",
      "Epoch 1/10 | Step 2250/2782 | Loss 0.4042\n",
      "Epoch 1/10 | Step 2300/2782 | Loss 0.3670\n",
      "Epoch 1/10 | Step 2350/2782 | Loss 0.1279\n",
      "Epoch 1/10 | Step 2400/2782 | Loss 0.6278\n",
      "Epoch 1/10 | Step 2450/2782 | Loss 0.3264\n",
      "Epoch 1/10 | Step 2500/2782 | Loss 0.3750\n",
      "Epoch 1/10 | Step 2550/2782 | Loss 0.3720\n",
      "Epoch 1/10 | Step 2600/2782 | Loss 0.2134\n",
      "Epoch 1/10 | Step 2650/2782 | Loss 0.6161\n",
      "Epoch 1/10 | Step 2700/2782 | Loss 0.3341\n",
      "Epoch 1/10 | Step 2750/2782 | Loss 0.2819\n",
      "Epoch 1/10 completed. Avg Loss: 0.4934\n",
      "Epoch 2/10 | Step 50/2782 | Loss 0.3220\n",
      "Epoch 2/10 | Step 100/2782 | Loss 0.2299\n",
      "Epoch 2/10 | Step 150/2782 | Loss 0.1727\n",
      "Epoch 2/10 | Step 200/2782 | Loss 0.3145\n",
      "Epoch 2/10 | Step 250/2782 | Loss 0.7973\n",
      "Epoch 2/10 | Step 300/2782 | Loss 0.4195\n",
      "Epoch 2/10 | Step 350/2782 | Loss 0.3340\n",
      "Epoch 2/10 | Step 400/2782 | Loss 0.2594\n",
      "Epoch 2/10 | Step 450/2782 | Loss 0.7737\n",
      "Epoch 2/10 | Step 500/2782 | Loss 0.1897\n",
      "Epoch 2/10 | Step 550/2782 | Loss 0.3482\n",
      "Epoch 2/10 | Step 600/2782 | Loss 0.1736\n",
      "Epoch 2/10 | Step 650/2782 | Loss 0.2078\n",
      "Epoch 2/10 | Step 700/2782 | Loss 0.0780\n",
      "Epoch 2/10 | Step 750/2782 | Loss 0.2095\n",
      "Epoch 2/10 | Step 800/2782 | Loss 0.2399\n",
      "Epoch 2/10 | Step 850/2782 | Loss 0.5831\n",
      "Epoch 2/10 | Step 900/2782 | Loss 0.1335\n",
      "Epoch 2/10 | Step 950/2782 | Loss 0.3621\n",
      "Epoch 2/10 | Step 1000/2782 | Loss 0.2918\n",
      "Epoch 2/10 | Step 1050/2782 | Loss 0.3809\n",
      "Epoch 2/10 | Step 1100/2782 | Loss 0.2237\n",
      "Epoch 2/10 | Step 1150/2782 | Loss 0.1031\n",
      "Epoch 2/10 | Step 1200/2782 | Loss 0.2625\n",
      "Epoch 2/10 | Step 1250/2782 | Loss 0.3075\n",
      "Epoch 2/10 | Step 1300/2782 | Loss 0.3856\n",
      "Epoch 2/10 | Step 1350/2782 | Loss 0.2265\n",
      "Epoch 2/10 | Step 1400/2782 | Loss 0.2638\n",
      "Epoch 2/10 | Step 1450/2782 | Loss 0.9686\n",
      "Epoch 2/10 | Step 1500/2782 | Loss 0.2531\n",
      "Epoch 2/10 | Step 1550/2782 | Loss 0.6134\n",
      "Epoch 2/10 | Step 1600/2782 | Loss 0.3679\n",
      "Epoch 2/10 | Step 1650/2782 | Loss 0.1795\n",
      "Epoch 2/10 | Step 1700/2782 | Loss 0.2414\n",
      "Epoch 2/10 | Step 1750/2782 | Loss 0.3015\n",
      "Epoch 2/10 | Step 1800/2782 | Loss 0.3145\n",
      "Epoch 2/10 | Step 1850/2782 | Loss 0.3918\n",
      "Epoch 2/10 | Step 1900/2782 | Loss 0.1977\n",
      "Epoch 2/10 | Step 1950/2782 | Loss 0.1260\n",
      "Epoch 2/10 | Step 2000/2782 | Loss 0.4392\n",
      "Epoch 2/10 | Step 2050/2782 | Loss 0.2581\n",
      "Epoch 2/10 | Step 2100/2782 | Loss 0.1472\n",
      "Epoch 2/10 | Step 2150/2782 | Loss 0.2641\n",
      "Epoch 2/10 | Step 2200/2782 | Loss 0.3359\n",
      "Epoch 2/10 | Step 2250/2782 | Loss 0.2528\n",
      "Epoch 2/10 | Step 2300/2782 | Loss 0.3268\n",
      "Epoch 2/10 | Step 2350/2782 | Loss 0.2223\n",
      "Epoch 2/10 | Step 2400/2782 | Loss 0.2435\n",
      "Epoch 2/10 | Step 2450/2782 | Loss 0.2592\n",
      "Epoch 2/10 | Step 2500/2782 | Loss 0.3484\n",
      "Epoch 2/10 | Step 2550/2782 | Loss 0.1679\n",
      "Epoch 2/10 | Step 2600/2782 | Loss 0.9752\n",
      "Epoch 2/10 | Step 2650/2782 | Loss 0.1232\n",
      "Epoch 2/10 | Step 2700/2782 | Loss 0.2278\n",
      "Epoch 2/10 | Step 2750/2782 | Loss 0.0798\n",
      "Epoch 2/10 completed. Avg Loss: 0.2993\n",
      "Epoch 3/10 | Step 50/2782 | Loss 0.4428\n",
      "Epoch 3/10 | Step 100/2782 | Loss 0.1430\n",
      "Epoch 3/10 | Step 150/2782 | Loss 0.1523\n",
      "Epoch 3/10 | Step 200/2782 | Loss 0.1661\n",
      "Epoch 3/10 | Step 250/2782 | Loss 0.2828\n",
      "Epoch 3/10 | Step 300/2782 | Loss 0.1427\n",
      "Epoch 3/10 | Step 350/2782 | Loss 0.1566\n",
      "Epoch 3/10 | Step 400/2782 | Loss 0.1651\n",
      "Epoch 3/10 | Step 450/2782 | Loss 0.2419\n",
      "Epoch 3/10 | Step 500/2782 | Loss 0.3554\n",
      "Epoch 3/10 | Step 550/2782 | Loss 0.0657\n",
      "Epoch 3/10 | Step 600/2782 | Loss 0.2086\n",
      "Epoch 3/10 | Step 650/2782 | Loss 0.1224\n",
      "Epoch 3/10 | Step 700/2782 | Loss 0.1013\n",
      "Epoch 3/10 | Step 750/2782 | Loss 0.2624\n",
      "Epoch 3/10 | Step 800/2782 | Loss 0.2340\n",
      "Epoch 3/10 | Step 850/2782 | Loss 0.3752\n",
      "Epoch 3/10 | Step 900/2782 | Loss 0.0917\n",
      "Epoch 3/10 | Step 950/2782 | Loss 0.2340\n",
      "Epoch 3/10 | Step 1000/2782 | Loss 0.2826\n",
      "Epoch 3/10 | Step 1050/2782 | Loss 0.1857\n",
      "Epoch 3/10 | Step 1100/2782 | Loss 0.2596\n",
      "Epoch 3/10 | Step 1150/2782 | Loss 0.2027\n",
      "Epoch 3/10 | Step 1200/2782 | Loss 0.1698\n",
      "Epoch 3/10 | Step 1250/2782 | Loss 0.2770\n",
      "Epoch 3/10 | Step 1300/2782 | Loss 0.1571\n",
      "Epoch 3/10 | Step 1350/2782 | Loss 0.1237\n",
      "Epoch 3/10 | Step 1400/2782 | Loss 0.1079\n",
      "Epoch 3/10 | Step 1450/2782 | Loss 0.2721\n",
      "Epoch 3/10 | Step 1500/2782 | Loss 0.3519\n",
      "Epoch 3/10 | Step 1550/2782 | Loss 0.1914\n",
      "Epoch 3/10 | Step 1600/2782 | Loss 0.1495\n",
      "Epoch 3/10 | Step 1650/2782 | Loss 0.2624\n",
      "Epoch 3/10 | Step 1700/2782 | Loss 0.2132\n",
      "Epoch 3/10 | Step 1750/2782 | Loss 0.7013\n",
      "Epoch 3/10 | Step 1800/2782 | Loss 0.1196\n",
      "Epoch 3/10 | Step 1850/2782 | Loss 0.4428\n",
      "Epoch 3/10 | Step 1900/2782 | Loss 0.1348\n",
      "Epoch 3/10 | Step 1950/2782 | Loss 0.1418\n",
      "Epoch 3/10 | Step 2000/2782 | Loss 0.1558\n",
      "Epoch 3/10 | Step 2050/2782 | Loss 0.1925\n",
      "Epoch 3/10 | Step 2100/2782 | Loss 0.3571\n",
      "Epoch 3/10 | Step 2150/2782 | Loss 0.0908\n",
      "Epoch 3/10 | Step 2200/2782 | Loss 0.1617\n",
      "Epoch 3/10 | Step 2250/2782 | Loss 0.1820\n",
      "Epoch 3/10 | Step 2300/2782 | Loss 0.1497\n",
      "Epoch 3/10 | Step 2350/2782 | Loss 0.1003\n",
      "Epoch 3/10 | Step 2400/2782 | Loss 0.1467\n",
      "Epoch 3/10 | Step 2450/2782 | Loss 0.1486\n",
      "Epoch 3/10 | Step 2500/2782 | Loss 0.1744\n",
      "Epoch 3/10 | Step 2550/2782 | Loss 0.1935\n",
      "Epoch 3/10 | Step 2600/2782 | Loss 0.1403\n",
      "Epoch 3/10 | Step 2650/2782 | Loss 0.1863\n",
      "Epoch 3/10 | Step 2700/2782 | Loss 0.0599\n",
      "Epoch 3/10 | Step 2750/2782 | Loss 0.1331\n",
      "Epoch 3/10 completed. Avg Loss: 0.2112\n",
      "Epoch 4/10 | Step 50/2782 | Loss 0.2385\n",
      "Epoch 4/10 | Step 100/2782 | Loss 0.3268\n",
      "Epoch 4/10 | Step 150/2782 | Loss 0.1286\n",
      "Epoch 4/10 | Step 200/2782 | Loss 0.1366\n",
      "Epoch 4/10 | Step 250/2782 | Loss 0.1175\n",
      "Epoch 4/10 | Step 300/2782 | Loss 0.2012\n",
      "Epoch 4/10 | Step 350/2782 | Loss 0.2045\n",
      "Epoch 4/10 | Step 400/2782 | Loss 0.1352\n",
      "Epoch 4/10 | Step 450/2782 | Loss 0.0946\n",
      "Epoch 4/10 | Step 500/2782 | Loss 0.0941\n",
      "Epoch 4/10 | Step 550/2782 | Loss 0.1235\n",
      "Epoch 4/10 | Step 600/2782 | Loss 0.0945\n",
      "Epoch 4/10 | Step 650/2782 | Loss 0.1289\n",
      "Epoch 4/10 | Step 700/2782 | Loss 0.3023\n",
      "Epoch 4/10 | Step 750/2782 | Loss 0.1433\n",
      "Epoch 4/10 | Step 800/2782 | Loss 0.1172\n",
      "Epoch 4/10 | Step 850/2782 | Loss 0.1429\n",
      "Epoch 4/10 | Step 900/2782 | Loss 0.1233\n",
      "Epoch 4/10 | Step 950/2782 | Loss 0.0763\n",
      "Epoch 4/10 | Step 1000/2782 | Loss 0.1655\n",
      "Epoch 4/10 | Step 1050/2782 | Loss 0.0986\n",
      "Epoch 4/10 | Step 1100/2782 | Loss 0.1382\n",
      "Epoch 4/10 | Step 1150/2782 | Loss 0.2013\n",
      "Epoch 4/10 | Step 1200/2782 | Loss 0.2492\n",
      "Epoch 4/10 | Step 1250/2782 | Loss 0.2040\n",
      "Epoch 4/10 | Step 1300/2782 | Loss 0.0506\n",
      "Epoch 4/10 | Step 1350/2782 | Loss 0.2063\n",
      "Epoch 4/10 | Step 1400/2782 | Loss 0.1240\n",
      "Epoch 4/10 | Step 1450/2782 | Loss 0.0880\n",
      "Epoch 4/10 | Step 1500/2782 | Loss 0.1446\n",
      "Epoch 4/10 | Step 1550/2782 | Loss 0.1237\n",
      "Epoch 4/10 | Step 1600/2782 | Loss 0.0503\n",
      "Epoch 4/10 | Step 1650/2782 | Loss 0.0917\n",
      "Epoch 4/10 | Step 1700/2782 | Loss 0.1336\n",
      "Epoch 4/10 | Step 1750/2782 | Loss 0.0657\n",
      "Epoch 4/10 | Step 1800/2782 | Loss 0.3136\n",
      "Epoch 4/10 | Step 1850/2782 | Loss 0.1553\n",
      "Epoch 4/10 | Step 1900/2782 | Loss 0.1133\n",
      "Epoch 4/10 | Step 1950/2782 | Loss 0.9059\n",
      "Epoch 4/10 | Step 2000/2782 | Loss 0.1545\n",
      "Epoch 4/10 | Step 2050/2782 | Loss 0.1278\n",
      "Epoch 4/10 | Step 2100/2782 | Loss 0.1002\n",
      "Epoch 4/10 | Step 2150/2782 | Loss 0.3037\n",
      "Epoch 4/10 | Step 2200/2782 | Loss 0.1288\n",
      "Epoch 4/10 | Step 2250/2782 | Loss 0.1053\n",
      "Epoch 4/10 | Step 2300/2782 | Loss 0.1611\n",
      "Epoch 4/10 | Step 2350/2782 | Loss 0.1182\n",
      "Epoch 4/10 | Step 2400/2782 | Loss 0.1298\n",
      "Epoch 4/10 | Step 2450/2782 | Loss 0.2702\n",
      "Epoch 4/10 | Step 2500/2782 | Loss 0.2203\n",
      "Epoch 4/10 | Step 2550/2782 | Loss 0.1321\n",
      "Epoch 4/10 | Step 2600/2782 | Loss 0.2070\n",
      "Epoch 4/10 | Step 2650/2782 | Loss 0.1209\n",
      "Epoch 4/10 | Step 2700/2782 | Loss 0.3332\n",
      "Epoch 4/10 | Step 2750/2782 | Loss 0.2599\n",
      "Epoch 4/10 completed. Avg Loss: 0.1654\n",
      "Epoch 5/10 | Step 50/2782 | Loss 0.1821\n",
      "Epoch 5/10 | Step 100/2782 | Loss 0.1725\n",
      "Epoch 5/10 | Step 150/2782 | Loss 0.0824\n",
      "Epoch 5/10 | Step 200/2782 | Loss 0.1875\n",
      "Epoch 5/10 | Step 250/2782 | Loss 0.2921\n",
      "Epoch 5/10 | Step 300/2782 | Loss 0.1331\n",
      "Epoch 5/10 | Step 350/2782 | Loss 0.1019\n",
      "Epoch 5/10 | Step 400/2782 | Loss 0.0830\n",
      "Epoch 5/10 | Step 450/2782 | Loss 0.1027\n",
      "Epoch 5/10 | Step 500/2782 | Loss 0.0947\n",
      "Epoch 5/10 | Step 550/2782 | Loss 0.1041\n",
      "Epoch 5/10 | Step 600/2782 | Loss 0.2065\n",
      "Epoch 5/10 | Step 650/2782 | Loss 0.0951\n",
      "Epoch 5/10 | Step 700/2782 | Loss 0.1286\n",
      "Epoch 5/10 | Step 750/2782 | Loss 0.0918\n",
      "Epoch 5/10 | Step 800/2782 | Loss 0.0745\n",
      "Epoch 5/10 | Step 850/2782 | Loss 0.1853\n",
      "Epoch 5/10 | Step 900/2782 | Loss 0.1014\n",
      "Epoch 5/10 | Step 950/2782 | Loss 0.1038\n",
      "Epoch 5/10 | Step 1000/2782 | Loss 0.0631\n",
      "Epoch 5/10 | Step 1050/2782 | Loss 0.1126\n",
      "Epoch 5/10 | Step 1100/2782 | Loss 0.1760\n",
      "Epoch 5/10 | Step 1150/2782 | Loss 0.1726\n",
      "Epoch 5/10 | Step 1200/2782 | Loss 0.2028\n",
      "Epoch 5/10 | Step 1250/2782 | Loss 0.0463\n",
      "Epoch 5/10 | Step 1300/2782 | Loss 0.1953\n",
      "Epoch 5/10 | Step 1350/2782 | Loss 0.1682\n",
      "Epoch 5/10 | Step 1400/2782 | Loss 0.2156\n",
      "Epoch 5/10 | Step 1450/2782 | Loss 0.0377\n",
      "Epoch 5/10 | Step 1500/2782 | Loss 0.0447\n",
      "Epoch 5/10 | Step 1550/2782 | Loss 0.0911\n",
      "Epoch 5/10 | Step 1600/2782 | Loss 0.1927\n",
      "Epoch 5/10 | Step 1650/2782 | Loss 0.0915\n",
      "Epoch 5/10 | Step 1700/2782 | Loss 0.5929\n",
      "Epoch 5/10 | Step 1750/2782 | Loss 0.0671\n",
      "Epoch 5/10 | Step 1800/2782 | Loss 0.1263\n",
      "Epoch 5/10 | Step 1850/2782 | Loss 0.3372\n",
      "Epoch 5/10 | Step 1900/2782 | Loss 0.1764\n",
      "Epoch 5/10 | Step 1950/2782 | Loss 0.1825\n",
      "Epoch 5/10 | Step 2000/2782 | Loss 0.0837\n",
      "Epoch 5/10 | Step 2050/2782 | Loss 0.1448\n",
      "Epoch 5/10 | Step 2100/2782 | Loss 0.1003\n",
      "Epoch 5/10 | Step 2150/2782 | Loss 0.1037\n",
      "Epoch 5/10 | Step 2200/2782 | Loss 0.1726\n",
      "Epoch 5/10 | Step 2250/2782 | Loss 0.2149\n",
      "Epoch 5/10 | Step 2300/2782 | Loss 0.1330\n",
      "Epoch 5/10 | Step 2350/2782 | Loss 0.0869\n",
      "Epoch 5/10 | Step 2400/2782 | Loss 0.0970\n",
      "Epoch 5/10 | Step 2450/2782 | Loss 0.3639\n",
      "Epoch 5/10 | Step 2500/2782 | Loss 0.0423\n",
      "Epoch 5/10 | Step 2550/2782 | Loss 0.0538\n",
      "Epoch 5/10 | Step 2600/2782 | Loss 0.0332\n",
      "Epoch 5/10 | Step 2650/2782 | Loss 0.0418\n",
      "Epoch 5/10 | Step 2700/2782 | Loss 0.2580\n",
      "Epoch 5/10 | Step 2750/2782 | Loss 0.0765\n",
      "Epoch 5/10 completed. Avg Loss: 0.1478\n",
      "Epoch 6/10 | Step 50/2782 | Loss 0.1254\n",
      "Epoch 6/10 | Step 100/2782 | Loss 0.1152\n",
      "Epoch 6/10 | Step 150/2782 | Loss 0.1736\n",
      "Epoch 6/10 | Step 200/2782 | Loss 0.1135\n",
      "Epoch 6/10 | Step 250/2782 | Loss 0.0199\n",
      "Epoch 6/10 | Step 300/2782 | Loss 0.1007\n",
      "Epoch 6/10 | Step 350/2782 | Loss 0.1912\n",
      "Epoch 6/10 | Step 400/2782 | Loss 0.1379\n",
      "Epoch 6/10 | Step 450/2782 | Loss 0.0990\n",
      "Epoch 6/10 | Step 500/2782 | Loss 0.1600\n",
      "Epoch 6/10 | Step 550/2782 | Loss 0.0780\n",
      "Epoch 6/10 | Step 600/2782 | Loss 0.1163\n",
      "Epoch 6/10 | Step 650/2782 | Loss 0.1346\n",
      "Epoch 6/10 | Step 700/2782 | Loss 0.1171\n",
      "Epoch 6/10 | Step 750/2782 | Loss 0.0401\n",
      "Epoch 6/10 | Step 800/2782 | Loss 0.0298\n",
      "Epoch 6/10 | Step 850/2782 | Loss 0.0770\n",
      "Epoch 6/10 | Step 900/2782 | Loss 0.0459\n",
      "Epoch 6/10 | Step 950/2782 | Loss 0.2251\n",
      "Epoch 6/10 | Step 1000/2782 | Loss 0.3644\n",
      "Epoch 6/10 | Step 1050/2782 | Loss 0.0516\n",
      "Epoch 6/10 | Step 1100/2782 | Loss 0.1714\n",
      "Epoch 6/10 | Step 1150/2782 | Loss 0.0855\n",
      "Epoch 6/10 | Step 1200/2782 | Loss 0.0746\n",
      "Epoch 6/10 | Step 1250/2782 | Loss 0.1293\n",
      "Epoch 6/10 | Step 1300/2782 | Loss 0.0667\n",
      "Epoch 6/10 | Step 1350/2782 | Loss 0.1662\n",
      "Epoch 6/10 | Step 1400/2782 | Loss 0.1838\n",
      "Epoch 6/10 | Step 1450/2782 | Loss 0.0563\n",
      "Epoch 6/10 | Step 1500/2782 | Loss 0.0528\n",
      "Epoch 6/10 | Step 1550/2782 | Loss 0.0220\n",
      "Epoch 6/10 | Step 1600/2782 | Loss 0.0244\n",
      "Epoch 6/10 | Step 1650/2782 | Loss 0.0784\n",
      "Epoch 6/10 | Step 1700/2782 | Loss 0.0849\n",
      "Epoch 6/10 | Step 1750/2782 | Loss 0.0722\n",
      "Epoch 6/10 | Step 1800/2782 | Loss 0.0431\n",
      "Epoch 6/10 | Step 1850/2782 | Loss 0.0370\n",
      "Epoch 6/10 | Step 1900/2782 | Loss 0.0323\n",
      "Epoch 6/10 | Step 1950/2782 | Loss 0.1382\n",
      "Epoch 6/10 | Step 2000/2782 | Loss 0.0130\n",
      "Epoch 6/10 | Step 2050/2782 | Loss 0.0458\n",
      "Epoch 6/10 | Step 2100/2782 | Loss 0.0534\n",
      "Epoch 6/10 | Step 2150/2782 | Loss 0.1563\n",
      "Epoch 6/10 | Step 2200/2782 | Loss 0.1120\n",
      "Epoch 6/10 | Step 2250/2782 | Loss 0.0629\n",
      "Epoch 6/10 | Step 2300/2782 | Loss 0.0230\n",
      "Epoch 6/10 | Step 2350/2782 | Loss 0.0659\n",
      "Epoch 6/10 | Step 2400/2782 | Loss 0.0893\n",
      "Epoch 6/10 | Step 2450/2782 | Loss 0.0752\n",
      "Epoch 6/10 | Step 2500/2782 | Loss 0.0556\n",
      "Epoch 6/10 | Step 2550/2782 | Loss 0.3629\n",
      "Epoch 6/10 | Step 2600/2782 | Loss 0.1237\n",
      "Epoch 6/10 | Step 2650/2782 | Loss 0.2035\n",
      "Epoch 6/10 | Step 2700/2782 | Loss 0.0790\n",
      "Epoch 6/10 | Step 2750/2782 | Loss 0.0575\n",
      "Epoch 6/10 completed. Avg Loss: 0.1268\n",
      "Epoch 7/10 | Step 50/2782 | Loss 0.0365\n",
      "Epoch 7/10 | Step 100/2782 | Loss 0.0786\n",
      "Epoch 7/10 | Step 150/2782 | Loss 0.0743\n",
      "Epoch 7/10 | Step 200/2782 | Loss 0.0497\n",
      "Epoch 7/10 | Step 250/2782 | Loss 0.1033\n",
      "Epoch 7/10 | Step 300/2782 | Loss 0.2079\n",
      "Epoch 7/10 | Step 350/2782 | Loss 0.2614\n",
      "Epoch 7/10 | Step 400/2782 | Loss 0.0581\n",
      "Epoch 7/10 | Step 450/2782 | Loss 0.0413\n",
      "Epoch 7/10 | Step 500/2782 | Loss 0.0601\n",
      "Epoch 7/10 | Step 550/2782 | Loss 0.1299\n",
      "Epoch 7/10 | Step 600/2782 | Loss 0.1223\n",
      "Epoch 7/10 | Step 650/2782 | Loss 0.1155\n",
      "Epoch 7/10 | Step 700/2782 | Loss 0.0523\n",
      "Epoch 7/10 | Step 750/2782 | Loss 0.0487\n",
      "Epoch 7/10 | Step 800/2782 | Loss 0.1964\n",
      "Epoch 7/10 | Step 850/2782 | Loss 0.1544\n",
      "Epoch 7/10 | Step 900/2782 | Loss 0.0985\n",
      "Epoch 7/10 | Step 950/2782 | Loss 0.0868\n",
      "Epoch 7/10 | Step 1000/2782 | Loss 0.1914\n",
      "Epoch 7/10 | Step 1050/2782 | Loss 0.4319\n",
      "Epoch 7/10 | Step 1100/2782 | Loss 0.1499\n",
      "Epoch 7/10 | Step 1150/2782 | Loss 0.0528\n",
      "Epoch 7/10 | Step 1200/2782 | Loss 0.1139\n",
      "Epoch 7/10 | Step 1250/2782 | Loss 0.0656\n",
      "Epoch 7/10 | Step 1300/2782 | Loss 0.0224\n",
      "Epoch 7/10 | Step 1350/2782 | Loss 0.0936\n",
      "Epoch 7/10 | Step 1400/2782 | Loss 0.0661\n",
      "Epoch 7/10 | Step 1450/2782 | Loss 0.0826\n",
      "Epoch 7/10 | Step 1500/2782 | Loss 0.0444\n",
      "Epoch 7/10 | Step 1550/2782 | Loss 0.1768\n",
      "Epoch 7/10 | Step 1600/2782 | Loss 0.0931\n",
      "Epoch 7/10 | Step 1650/2782 | Loss 0.0906\n",
      "Epoch 7/10 | Step 1700/2782 | Loss 0.1187\n",
      "Epoch 7/10 | Step 1750/2782 | Loss 0.0973\n",
      "Epoch 7/10 | Step 1800/2782 | Loss 0.0612\n",
      "Epoch 7/10 | Step 1850/2782 | Loss 0.0577\n",
      "Epoch 7/10 | Step 1900/2782 | Loss 0.1345\n",
      "Epoch 7/10 | Step 1950/2782 | Loss 0.1263\n",
      "Epoch 7/10 | Step 2000/2782 | Loss 0.0724\n",
      "Epoch 7/10 | Step 2050/2782 | Loss 0.2019\n",
      "Epoch 7/10 | Step 2100/2782 | Loss 0.0207\n",
      "Epoch 7/10 | Step 2150/2782 | Loss 0.0308\n",
      "Epoch 7/10 | Step 2200/2782 | Loss 0.0379\n",
      "Epoch 7/10 | Step 2250/2782 | Loss 0.0918\n",
      "Epoch 7/10 | Step 2300/2782 | Loss 0.0835\n",
      "Epoch 7/10 | Step 2350/2782 | Loss 0.0432\n",
      "Epoch 7/10 | Step 2400/2782 | Loss 0.0415\n",
      "Epoch 7/10 | Step 2450/2782 | Loss 0.0464\n",
      "Epoch 7/10 | Step 2500/2782 | Loss 0.0451\n",
      "Epoch 7/10 | Step 2550/2782 | Loss 0.1332\n",
      "Epoch 7/10 | Step 2600/2782 | Loss 0.0548\n",
      "Epoch 7/10 | Step 2650/2782 | Loss 0.1131\n",
      "Epoch 7/10 | Step 2700/2782 | Loss 0.1085\n",
      "Epoch 7/10 | Step 2750/2782 | Loss 0.1014\n",
      "Epoch 7/10 completed. Avg Loss: 0.1131\n",
      "Epoch 8/10 | Step 50/2782 | Loss 0.0359\n",
      "Epoch 8/10 | Step 100/2782 | Loss 0.0310\n",
      "Epoch 8/10 | Step 150/2782 | Loss 0.0604\n",
      "Epoch 8/10 | Step 200/2782 | Loss 0.1858\n",
      "Epoch 8/10 | Step 250/2782 | Loss 0.0552\n",
      "Epoch 8/10 | Step 300/2782 | Loss 0.0297\n",
      "Epoch 8/10 | Step 350/2782 | Loss 0.0384\n",
      "Epoch 8/10 | Step 400/2782 | Loss 0.0907\n",
      "Epoch 8/10 | Step 450/2782 | Loss 0.0593\n",
      "Epoch 8/10 | Step 500/2782 | Loss 0.2502\n",
      "Epoch 8/10 | Step 550/2782 | Loss 0.0631\n",
      "Epoch 8/10 | Step 600/2782 | Loss 0.1263\n",
      "Epoch 8/10 | Step 650/2782 | Loss 0.2752\n",
      "Epoch 8/10 | Step 700/2782 | Loss 0.0963\n",
      "Epoch 8/10 | Step 750/2782 | Loss 0.0461\n",
      "Epoch 8/10 | Step 800/2782 | Loss 0.0770\n",
      "Epoch 8/10 | Step 850/2782 | Loss 0.1115\n",
      "Epoch 8/10 | Step 900/2782 | Loss 0.0422\n",
      "Epoch 8/10 | Step 950/2782 | Loss 0.0628\n",
      "Epoch 8/10 | Step 1000/2782 | Loss 0.0641\n",
      "Epoch 8/10 | Step 1050/2782 | Loss 0.0606\n",
      "Epoch 8/10 | Step 1100/2782 | Loss 0.1599\n",
      "Epoch 8/10 | Step 1150/2782 | Loss 0.1594\n",
      "Epoch 8/10 | Step 1200/2782 | Loss 0.0315\n",
      "Epoch 8/10 | Step 1250/2782 | Loss 0.0955\n",
      "Epoch 8/10 | Step 1300/2782 | Loss 0.0555\n",
      "Epoch 8/10 | Step 1350/2782 | Loss 0.0829\n",
      "Epoch 8/10 | Step 1400/2782 | Loss 0.0593\n",
      "Epoch 8/10 | Step 1450/2782 | Loss 0.1170\n",
      "Epoch 8/10 | Step 1500/2782 | Loss 0.0143\n",
      "Epoch 8/10 | Step 1550/2782 | Loss 0.0715\n",
      "Epoch 8/10 | Step 1600/2782 | Loss 0.3699\n",
      "Epoch 8/10 | Step 1650/2782 | Loss 0.1423\n",
      "Epoch 8/10 | Step 1700/2782 | Loss 0.2262\n",
      "Epoch 8/10 | Step 1750/2782 | Loss 0.0666\n",
      "Epoch 8/10 | Step 1800/2782 | Loss 0.0791\n",
      "Epoch 8/10 | Step 1850/2782 | Loss 0.1512\n",
      "Epoch 8/10 | Step 1900/2782 | Loss 0.1292\n",
      "Epoch 8/10 | Step 1950/2782 | Loss 0.3608\n",
      "Epoch 8/10 | Step 2000/2782 | Loss 0.0739\n",
      "Epoch 8/10 | Step 2050/2782 | Loss 0.0505\n",
      "Epoch 8/10 | Step 2100/2782 | Loss 0.1353\n",
      "Epoch 8/10 | Step 2150/2782 | Loss 0.0392\n",
      "Epoch 8/10 | Step 2200/2782 | Loss 0.0603\n",
      "Epoch 8/10 | Step 2250/2782 | Loss 0.0747\n",
      "Epoch 8/10 | Step 2300/2782 | Loss 0.0946\n",
      "Epoch 8/10 | Step 2350/2782 | Loss 0.0606\n",
      "Epoch 8/10 | Step 2400/2782 | Loss 0.1113\n",
      "Epoch 8/10 | Step 2450/2782 | Loss 0.0862\n",
      "Epoch 8/10 | Step 2500/2782 | Loss 0.0929\n",
      "Epoch 8/10 | Step 2550/2782 | Loss 0.0653\n",
      "Epoch 8/10 | Step 2600/2782 | Loss 0.6003\n",
      "Epoch 8/10 | Step 2650/2782 | Loss 0.0212\n",
      "Epoch 8/10 | Step 2700/2782 | Loss 1.4239\n",
      "Epoch 8/10 | Step 2750/2782 | Loss 0.0648\n",
      "Epoch 8/10 completed. Avg Loss: 0.1047\n",
      "Epoch 9/10 | Step 50/2782 | Loss 0.0434\n",
      "Epoch 9/10 | Step 100/2782 | Loss 0.0577\n",
      "Epoch 9/10 | Step 150/2782 | Loss 0.0459\n",
      "Epoch 9/10 | Step 200/2782 | Loss 0.0864\n",
      "Epoch 9/10 | Step 250/2782 | Loss 0.0977\n",
      "Epoch 9/10 | Step 300/2782 | Loss 0.0861\n",
      "Epoch 9/10 | Step 350/2782 | Loss 0.0661\n",
      "Epoch 9/10 | Step 400/2782 | Loss 0.0380\n",
      "Epoch 9/10 | Step 450/2782 | Loss 0.0550\n",
      "Epoch 9/10 | Step 500/2782 | Loss 0.1015\n",
      "Epoch 9/10 | Step 550/2782 | Loss 0.0308\n",
      "Epoch 9/10 | Step 600/2782 | Loss 0.0712\n",
      "Epoch 9/10 | Step 650/2782 | Loss 0.0627\n",
      "Epoch 9/10 | Step 700/2782 | Loss 0.0584\n",
      "Epoch 9/10 | Step 750/2782 | Loss 0.2100\n",
      "Epoch 9/10 | Step 800/2782 | Loss 0.1137\n",
      "Epoch 9/10 | Step 850/2782 | Loss 0.0584\n",
      "Epoch 9/10 | Step 900/2782 | Loss 0.0319\n",
      "Epoch 9/10 | Step 950/2782 | Loss 0.0282\n",
      "Epoch 9/10 | Step 1000/2782 | Loss 0.1390\n",
      "Epoch 9/10 | Step 1050/2782 | Loss 0.0775\n",
      "Epoch 9/10 | Step 1100/2782 | Loss 0.0461\n",
      "Epoch 9/10 | Step 1150/2782 | Loss 0.2640\n",
      "Epoch 9/10 | Step 1200/2782 | Loss 0.0925\n",
      "Epoch 9/10 | Step 1250/2782 | Loss 0.0295\n",
      "Epoch 9/10 | Step 1300/2782 | Loss 0.0562\n",
      "Epoch 9/10 | Step 1350/2782 | Loss 0.0883\n",
      "Epoch 9/10 | Step 1400/2782 | Loss 0.1832\n",
      "Epoch 9/10 | Step 1450/2782 | Loss 0.1172\n",
      "Epoch 9/10 | Step 1500/2782 | Loss 0.0409\n",
      "Epoch 9/10 | Step 1550/2782 | Loss 0.0938\n",
      "Epoch 9/10 | Step 1600/2782 | Loss 0.0519\n",
      "Epoch 9/10 | Step 1650/2782 | Loss 0.0722\n",
      "Epoch 9/10 | Step 1700/2782 | Loss 0.1177\n",
      "Epoch 9/10 | Step 1750/2782 | Loss 0.3094\n",
      "Epoch 9/10 | Step 1800/2782 | Loss 0.1034\n",
      "Epoch 9/10 | Step 1850/2782 | Loss 0.0797\n",
      "Epoch 9/10 | Step 1900/2782 | Loss 0.3665\n",
      "Epoch 9/10 | Step 1950/2782 | Loss 0.0721\n",
      "Epoch 9/10 | Step 2000/2782 | Loss 0.0264\n",
      "Epoch 9/10 | Step 2050/2782 | Loss 0.0736\n",
      "Epoch 9/10 | Step 2100/2782 | Loss 0.1767\n",
      "Epoch 9/10 | Step 2150/2782 | Loss 0.0695\n",
      "Epoch 9/10 | Step 2200/2782 | Loss 0.0949\n",
      "Epoch 9/10 | Step 2250/2782 | Loss 0.0781\n",
      "Epoch 9/10 | Step 2300/2782 | Loss 0.0859\n",
      "Epoch 9/10 | Step 2350/2782 | Loss 0.1241\n",
      "Epoch 9/10 | Step 2400/2782 | Loss 0.2401\n",
      "Epoch 9/10 | Step 2450/2782 | Loss 0.0605\n",
      "Epoch 9/10 | Step 2500/2782 | Loss 0.1797\n",
      "Epoch 9/10 | Step 2550/2782 | Loss 0.0990\n",
      "Epoch 9/10 | Step 2600/2782 | Loss 0.0568\n",
      "Epoch 9/10 | Step 2650/2782 | Loss 0.0943\n",
      "Epoch 9/10 | Step 2700/2782 | Loss 0.0418\n",
      "Epoch 9/10 | Step 2750/2782 | Loss 0.1490\n",
      "Epoch 9/10 completed. Avg Loss: 0.0995\n",
      "Epoch 10/10 | Step 50/2782 | Loss 0.1435\n",
      "Epoch 10/10 | Step 100/2782 | Loss 0.0503\n",
      "Epoch 10/10 | Step 150/2782 | Loss 0.0736\n",
      "Epoch 10/10 | Step 200/2782 | Loss 0.0506\n",
      "Epoch 10/10 | Step 250/2782 | Loss 0.0192\n",
      "Epoch 10/10 | Step 300/2782 | Loss 0.0767\n",
      "Epoch 10/10 | Step 350/2782 | Loss 0.0615\n",
      "Epoch 10/10 | Step 400/2782 | Loss 0.0237\n",
      "Epoch 10/10 | Step 450/2782 | Loss 0.1663\n",
      "Epoch 10/10 | Step 500/2782 | Loss 0.2187\n",
      "Epoch 10/10 | Step 550/2782 | Loss 0.0967\n",
      "Epoch 10/10 | Step 600/2782 | Loss 0.0752\n",
      "Epoch 10/10 | Step 650/2782 | Loss 0.0561\n",
      "Epoch 10/10 | Step 700/2782 | Loss 0.0471\n",
      "Epoch 10/10 | Step 750/2782 | Loss 0.0556\n",
      "Epoch 10/10 | Step 800/2782 | Loss 0.0126\n",
      "Epoch 10/10 | Step 850/2782 | Loss 0.0730\n",
      "Epoch 10/10 | Step 900/2782 | Loss 0.1364\n",
      "Epoch 10/10 | Step 950/2782 | Loss 0.0537\n",
      "Epoch 10/10 | Step 1000/2782 | Loss 0.0451\n",
      "Epoch 10/10 | Step 1050/2782 | Loss 0.0540\n",
      "Epoch 10/10 | Step 1100/2782 | Loss 0.0599\n",
      "Epoch 10/10 | Step 1150/2782 | Loss 0.8082\n",
      "Epoch 10/10 | Step 1200/2782 | Loss 0.0909\n",
      "Epoch 10/10 | Step 1250/2782 | Loss 0.0766\n",
      "Epoch 10/10 | Step 1300/2782 | Loss 0.2234\n",
      "Epoch 10/10 | Step 1350/2782 | Loss 0.0729\n",
      "Epoch 10/10 | Step 1400/2782 | Loss 0.0571\n",
      "Epoch 10/10 | Step 1450/2782 | Loss 0.1338\n",
      "Epoch 10/10 | Step 1500/2782 | Loss 0.0715\n",
      "Epoch 10/10 | Step 1550/2782 | Loss 0.1356\n",
      "Epoch 10/10 | Step 1600/2782 | Loss 0.1901\n",
      "Epoch 10/10 | Step 1650/2782 | Loss 0.0747\n",
      "Epoch 10/10 | Step 1700/2782 | Loss 0.1769\n",
      "Epoch 10/10 | Step 1750/2782 | Loss 0.0849\n",
      "Epoch 10/10 | Step 1800/2782 | Loss 0.0844\n",
      "Epoch 10/10 | Step 1850/2782 | Loss 0.0110\n",
      "Epoch 10/10 | Step 1900/2782 | Loss 0.0392\n",
      "Epoch 10/10 | Step 1950/2782 | Loss 0.0673\n",
      "Epoch 10/10 | Step 2000/2782 | Loss 0.0629\n",
      "Epoch 10/10 | Step 2050/2782 | Loss 0.0603\n",
      "Epoch 10/10 | Step 2100/2782 | Loss 0.0296\n",
      "Epoch 10/10 | Step 2150/2782 | Loss 0.0238\n",
      "Epoch 10/10 | Step 2200/2782 | Loss 0.0634\n",
      "Epoch 10/10 | Step 2250/2782 | Loss 0.0350\n",
      "Epoch 10/10 | Step 2300/2782 | Loss 0.6203\n",
      "Epoch 10/10 | Step 2350/2782 | Loss 0.1188\n",
      "Epoch 10/10 | Step 2400/2782 | Loss 0.0428\n",
      "Epoch 10/10 | Step 2450/2782 | Loss 0.0939\n",
      "Epoch 10/10 | Step 2500/2782 | Loss 0.0426\n",
      "Epoch 10/10 | Step 2550/2782 | Loss 0.0606\n",
      "Epoch 10/10 | Step 2600/2782 | Loss 0.0600\n",
      "Epoch 10/10 | Step 2650/2782 | Loss 0.0385\n",
      "Epoch 10/10 | Step 2700/2782 | Loss 0.0343\n",
      "Epoch 10/10 | Step 2750/2782 | Loss 0.0386\n",
      "Epoch 10/10 completed. Avg Loss: 0.0946\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 7) Training (AMP, cached GNN embeddings)\n",
    "# -------------------------\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "print(\"Begin training...\")\n",
    "\n",
    "def compute_gnn_embeddings_once(model_obj, graph_data):\n",
    "    \"\"\"\n",
    "    Compute and return GNN embeddings while preserving the model's original\n",
    "    training/eval state so we don't accidentally leave RNNs in eval mode.\n",
    "    \"\"\"\n",
    "    was_training = model_obj.training  # save original state\n",
    "    try:\n",
    "        model_obj.eval()\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            if isinstance(model_obj, nn.DataParallel):\n",
    "                g_emb = model_obj.module.gnn(graph_data.x, graph_data.edge_index)\n",
    "            else:\n",
    "                g_emb = model_obj.gnn(graph_data.x, graph_data.edge_index)\n",
    "    finally:\n",
    "        # restore original mode\n",
    "        if was_training:\n",
    "            model_obj.train()\n",
    "        else:\n",
    "            model_obj.eval()\n",
    "    return g_emb  # (n_nodes, gnn_out_dim)\n",
    "\n",
    "# prepare graphs on device\n",
    "train_graph_data = GeoData(x=train_graph.x.to(DEVICE), edge_index=train_graph.edge_index.to(DEVICE))\n",
    "test_graph_data  = GeoData(x=test_graph.x.to(DEVICE),  edge_index=test_graph.edge_index.to(DEVICE))\n",
    "\n",
    "# helper to forward using cached gnn embeddings (so we don't run gnn conv every batch)\n",
    "def forward_with_cached_lstm(model_obj, cached_gnn_embeds, seq_batch, host_idx, seq_anom):\n",
    "    # cached_gnn_embeds: (n_nodes, gnn_out_dim)\n",
    "    node_embeds = cached_gnn_embeds[host_idx]  # gather\n",
    "    seq_len = seq_batch.shape[1]\n",
    "    node_expanded = node_embeds.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "    if seq_anom.numel() > 1:\n",
    "        norm = (seq_anom - seq_anom.min()) / (seq_anom.max() - seq_anom.min() + 1e-6)\n",
    "    else:\n",
    "        norm = seq_anom * 0.0\n",
    "    weights = (1.0 + ANOMALY_WEIGHT_ALPHA * norm).unsqueeze(-1).unsqueeze(-1)\n",
    "    node_expanded = node_expanded * weights.to(node_expanded.device)\n",
    "    seq_context = torch.cat([seq_batch, node_expanded.to(seq_batch.dtype)], dim=-1)\n",
    "    # pass through only LSTM component\n",
    "    if isinstance(model_obj, nn.DataParallel):\n",
    "        logits, attn_w, lstm_outs = model_obj.module.lstm(seq_context)\n",
    "    else:\n",
    "        logits, attn_w, lstm_outs = model_obj.lstm(seq_context)\n",
    "    return logits, attn_w, lstm_outs\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()                 # ensure training mode at epoch start\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # compute cached gnn embeddings (this function now restores model state)\n",
    "    cached_train_gnn = compute_gnn_embeddings_once(model, train_graph_data)\n",
    "\n",
    "    for batch_idx, (batch_x, batch_y, batch_host_idx, batch_seq_anom) in enumerate(train_loader):\n",
    "        # move to device etc.\n",
    "        batch_x = batch_x.to(DEVICE, non_blocking=True).float()\n",
    "        batch_y = batch_y.to(DEVICE, non_blocking=True).long()\n",
    "        batch_host_idx = batch_host_idx.to(DEVICE, non_blocking=True).long()\n",
    "        batch_seq_anom = batch_seq_anom.to(DEVICE, non_blocking=True).float()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits, _, _ = forward_with_cached_lstm(model, cached_train_gnn, batch_x, batch_host_idx, batch_seq_anom)\n",
    "            loss = criterion(logits, batch_y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if PRINT_PROGRESS and (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS} | Step {batch_idx+1}/{len(train_loader)} | Loss {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(train_loader))\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} completed. Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "397e4602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Overall Test Metrics ===\n",
      "Accuracy: 0.9836\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.99      0.99      0.99    172447\n",
      "     Stealth       0.74      0.74      0.74      5604\n",
      "\n",
      "    accuracy                           0.98    178051\n",
      "   macro avg       0.87      0.86      0.87    178051\n",
      "weighted avg       0.98      0.98      0.98    178051\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-host summary (head):\n",
      "   host_node_idx  n_sequences  true_compromised  pred_compromised  \\\n",
      "0              0         1253                 1                 1   \n",
      "1              1         2210                 0                 0   \n",
      "2              2         2016                 0                 0   \n",
      "3              3         2451                 0                 0   \n",
      "4              4         1301                 0                 0   \n",
      "5              5         1144                 0                 0   \n",
      "6              6         1672                 0                 0   \n",
      "7              7         2780                 1                 0   \n",
      "8              8         1263                 1                 1   \n",
      "9              9         1377                 0                 0   \n",
      "\n",
      "   detection_rate_for_host  \n",
      "0                 0.056664  \n",
      "1                 0.000452  \n",
      "2                 0.000496  \n",
      "3                 0.000408  \n",
      "4                 0.000000  \n",
      "5                 0.000000  \n",
      "6                 0.000000  \n",
      "7                 0.000000  \n",
      "8                 0.051465  \n",
      "9                 0.000000  \n",
      "\n",
      "Per-host counts: TP=30, FN=10, FP=0, TN=60\n",
      "Model saved to slm_enhanced_model.pth\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 8) Evaluation (test)\n",
    "# -------------------------\n",
    "model.eval()\n",
    "all_preds, all_true, all_hosts = [], [], []\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    cached_test_gnn = compute_gnn_embeddings_once(model, test_graph_data)\n",
    "    for batch_x, batch_y, batch_host_idx, batch_seq_anom in test_loader:\n",
    "        batch_x = batch_x.to(DEVICE, non_blocking=True).float()\n",
    "        batch_y = batch_y.to(DEVICE, non_blocking=True).long()\n",
    "        batch_host_idx = batch_host_idx.to(DEVICE, non_blocking=True).long()\n",
    "        batch_seq_anom = batch_seq_anom.to(DEVICE, non_blocking=True).float()\n",
    "\n",
    "        logits, attn_w, _ = forward_with_cached_lstm(model, cached_test_gnn, batch_x, batch_host_idx, batch_seq_anom)\n",
    "        preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_true.extend(batch_y.cpu().numpy().tolist())\n",
    "        all_hosts.extend(batch_host_idx.cpu().numpy().tolist())\n",
    "\n",
    "all_preds = np.array(all_preds); all_true = np.array(all_true); all_hosts = np.array(all_hosts)\n",
    "acc = accuracy_score(all_true, all_preds)\n",
    "print(\"\\n=== Overall Test Metrics ===\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(classification_report(all_true, all_preds, target_names=[\"Benign\", \"Stealth\"], zero_division=0))\n",
    "\n",
    "cm = confusion_matrix(all_true, all_preds)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Benign\",\"Stealth\"], yticklabels=[\"Benign\",\"Stealth\"])\n",
    "plt.title(\"Confusion Matrix - Test\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Per-host evaluation\n",
    "per_host_stats = []\n",
    "unique_hosts = np.unique(all_hosts)\n",
    "for h in unique_hosts:\n",
    "    idxs = np.where(all_hosts == h)[0]\n",
    "    # True label for host: same rule (any stealth sequence ‚Üí stealth host)\n",
    "    t = int(all_true[idxs].max() > 0)\n",
    "    # Prediction for host using threshold\n",
    "    seq_pred = all_preds[idxs]\n",
    "    detection_rate = np.mean(seq_pred)   # % of sequences predicted as stealth\n",
    "    p = int(detection_rate >= THRESHOLD) # threshold decision\n",
    "    per_host_stats.append({\n",
    "        'host_node_idx': int(h),\n",
    "        'n_sequences': len(idxs),\n",
    "        'true_compromised': t,\n",
    "        'pred_compromised': p,\n",
    "        'detection_rate_for_host': float(detection_rate),\n",
    "    })\n",
    "\n",
    "\n",
    "per_host_df = pd.DataFrame(per_host_stats)\n",
    "print(\"\\nPer-host summary (head):\")\n",
    "print(per_host_df.head(10))\n",
    "tp = ((per_host_df['true_compromised']==1) & (per_host_df['pred_compromised']==1)).sum()\n",
    "fn = ((per_host_df['true_compromised']==1) & (per_host_df['pred_compromised']==0)).sum()\n",
    "fp = ((per_host_df['true_compromised']==0) & (per_host_df['pred_compromised']==1)).sum()\n",
    "tn = ((per_host_df['true_compromised']==0) & (per_host_df['pred_compromised']==0)).sum()\n",
    "print(f\"\\nPer-host counts: TP={tp}, FN={fn}, FP={fp}, TN={tn}\")\n",
    "\n",
    "# Save model weights (best-practice: track val metric and save best; here we save final)\n",
    "try:\n",
    "    model_to_save = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "    torch.save(model_to_save, SAVE_MODEL_PATH)\n",
    "    print(f\"Model saved to {SAVE_MODEL_PATH}\")\n",
    "except Exception as e:\n",
    "    print(\"Model save failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "284dc2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explainability sample failed: Tensors must have same number of dimensions: got 3 and 5\n",
      "\n",
      "Pipeline done.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 9) Explainability helper: Integrated Gradients (Captum) or gradient*input fallback\n",
    "# -------------------------\n",
    "def compute_sequence_attribution(\n",
    "        model_obj,\n",
    "        graph_data,\n",
    "        seq_tensor,\n",
    "        host_node_idx_tensor,\n",
    "        seq_anom_tensor,\n",
    "        target_label=1,\n",
    "        steps=50\n",
    "    ):\n",
    "    \"\"\"\n",
    "    seq_tensor: Tensor [1, seq_len, feat_dim]\n",
    "    host_node_idx_tensor: Tensor [1]\n",
    "    seq_anom_tensor: Tensor [1] or [1, 1] ‚Üí expanded to [1, seq_len, 1]\n",
    "    returns dict with 'seq_attr' (1, seq_len, feat_dim) and metadata\n",
    "    \"\"\"\n",
    "\n",
    "    model_obj.eval()\n",
    "\n",
    "    # ----------- Prepare tensors -----------\n",
    "    seq_tensor = seq_tensor.to(DEVICE).float().requires_grad_(True)\n",
    "    host_node_idx_tensor = host_node_idx_tensor.to(DEVICE).long()\n",
    "    seq_anom_tensor = seq_anom_tensor.to(DEVICE).float()\n",
    "\n",
    "    # üî• FIX: Expand anomaly scalar to match sequence length\n",
    "    # seq_anom_tensor comes as shape [1] or [1,1]\n",
    "    if seq_anom_tensor.dim() == 1:\n",
    "        # [1] ‚Üí [1,1]\n",
    "        seq_anom_tensor = seq_anom_tensor.unsqueeze(1)\n",
    "\n",
    "    seq_len = seq_tensor.shape[1]  # number of timesteps\n",
    "    # [1,1] ‚Üí [1,seq_len] ‚Üí [1,seq_len,1]\n",
    "    seq_anom_tensor = seq_anom_tensor.repeat(1, seq_len).unsqueeze(-1)\n",
    "\n",
    "    # Now safe to pass to model\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # Prepare graph data\n",
    "    graph_data = GeoData(\n",
    "        x=graph_data.x.to(DEVICE),\n",
    "        edge_index=graph_data.edge_index.to(DEVICE)\n",
    "    )\n",
    "\n",
    "    # Compute cached GNN embeddings once\n",
    "    cached_gnn = compute_gnn_embeddings_once(model_obj, graph_data)\n",
    "\n",
    "    # ----------- Forward wrapper for IG -----------\n",
    "    def forward_scalar(input_seq):\n",
    "        logits, _, _ = forward_with_cached_lstm(\n",
    "            model_obj,\n",
    "            cached_gnn,\n",
    "            input_seq,\n",
    "            host_node_idx_tensor,\n",
    "            seq_anom_tensor\n",
    "        )\n",
    "        return logits[:, target_label]\n",
    "\n",
    "    # ----------- Integrated Gradients or fallback -----------\n",
    "    if HAS_CAPTUM:\n",
    "        ig = IntegratedGradients(forward_scalar)\n",
    "        attributions, delta = ig.attribute(\n",
    "            seq_tensor,\n",
    "            n_steps=steps,\n",
    "            return_convergence_delta=True\n",
    "        )\n",
    "        return {\n",
    "            \"seq_attr\": attributions.detach().cpu().numpy(),\n",
    "            \"delta\": delta\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        # fallback: grad * input\n",
    "        logits, _, _ = forward_with_cached_lstm(\n",
    "            model_obj,\n",
    "            cached_gnn,\n",
    "            seq_tensor,\n",
    "            host_node_idx_tensor,\n",
    "            seq_anom_tensor\n",
    "        )\n",
    "        prob = logits[:, target_label].sum()\n",
    "        prob.backward()\n",
    "\n",
    "        grads = seq_tensor.grad\n",
    "        attr = (grads * seq_tensor).detach().cpu().numpy()\n",
    "\n",
    "        return {\"seq_attr\": attr, \"note\": \"grad*input fallback\"}\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Example usage on first test sample\n",
    "# -------------------------\n",
    "try:\n",
    "    if X_test_seq.shape[0] > 0:\n",
    "        sample_seq = X_test_seq[0:1]              # [1, seq_len, feat_dim]\n",
    "        sample_host_idx = test_host_node_idxs[0:1]  # [1]\n",
    "        sample_seq_anom = test_seq_anom_mean[0:1]   # [1] ‚Üí will be expanded inside function\n",
    "\n",
    "        explain_res = compute_sequence_attribution(\n",
    "            model,\n",
    "            test_graph,\n",
    "            sample_seq,\n",
    "            sample_host_idx,\n",
    "            sample_seq_anom\n",
    "        )\n",
    "\n",
    "        print(\"Explainability seq attr shape:\", explain_res.get(\"seq_attr\").shape)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Explainability sample failed:\", e)\n",
    "\n",
    "print(\"\\nPipeline done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40034f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All components saved to: saved_models/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "slm_train_and_save.py\n",
    "Trains the full Stealth Lateral Movement detection model and saves all artifacts for inference.\n",
    "\"\"\"\n",
    "\n",
    "import os, joblib, torch\n",
    "\n",
    "# ==========================================================\n",
    "# After training finishes (assume 'model', 'iso', 'scaler', 'encoders', 'train_graph_data' exist)\n",
    "# ==========================================================\n",
    "SAVE_DIR = \"saved_models\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = os.path.join(SAVE_DIR, \"slm_enhanced_model.pth\")\n",
    "ENCODERS_PATH = os.path.join(SAVE_DIR, \"label_encoders.pkl\")\n",
    "SCALER_PATH = os.path.join(SAVE_DIR, \"scaler.pkl\")\n",
    "GNN_CACHE_PATH = os.path.join(SAVE_DIR, \"cached_train_gnn.pt\")\n",
    "ANOMALY_MODEL_PATH = os.path.join(SAVE_DIR, \"isolation_forest.pkl\")\n",
    "\n",
    "try:\n",
    "    # Save unified neural model (GNN + BiLSTM + Attention)\n",
    "    model_to_save = model.module.state_dict() if isinstance(model, torch.nn.DataParallel) else model.state_dict()\n",
    "    torch.save(model_to_save, MODEL_PATH)\n",
    "\n",
    "    # Save all preprocessing + anomaly model\n",
    "    joblib.dump(encoders, ENCODERS_PATH)\n",
    "    joblib.dump(scaler, SCALER_PATH)\n",
    "    joblib.dump(iso, ANOMALY_MODEL_PATH)\n",
    "\n",
    "    # Save cached GNN embeddings (optional, speeds up inference)\n",
    "    cached_gnn = compute_gnn_embeddings_once(model, train_graph_data)\n",
    "    torch.save(cached_gnn, GNN_CACHE_PATH)\n",
    "\n",
    "    print(f\"‚úÖ All components saved to: {SAVE_DIR}/\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Model save failed:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
